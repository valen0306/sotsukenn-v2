#!/usr/bin/env python3
"""
TypeBERT inference adapter (Phase3 DTS_MODEL).

This script is intentionally an adapter layer:
- It defines a stable JSON I/O contract for the evaluation runner.
- You can swap the backend implementation later (actual TypeBERT weights/checkpoint).

Current default backend is a safe placeholder that emits "any-typed" declarations
based on requested modules/imported names. This lets you validate the end-to-end
pipeline (extract -> infer -> inject -> tsc -> aggregate) before the real model is wired.

Backends:
- stub: safe placeholder that emits "any-typed" declarations.
- hf_causal_lm: local HuggingFace/Transformers checkpoint-based generation backend
  (works with instruction-tuned causal LM checkpoints such as Qwen/StarCoder2/DeepSeek-Coder, etc.)
- typebert: alias for hf_causal_lm (kept for backward compatibility with earlier experiments/flags).
  - If deps/checkpoint are missing, it falls back to stub but still returns ok=true.

Input (stdin JSON):
{
  "repo": {"url": "...", "slug": "..."},
  "modules": {
    "pkg": {
      "defaultImport": true|false,
      "named": ["foo", "bar"],
      "typeNamed": ["Baz", "Qux"],
      "members": { "Foo": ["bar", "baz"] } // optional: named import member access (Foo.bar)
    },
    ...
  }
}

Output (stdout JSON):
{
  "ok": true,
  "backend": "hf_causal_lm"|"typebert"|"stub",
  "dts": "declare module 'pkg' { ... }\\n...",
  "meta": { ... },
  "cache_key": "..." // present when --cache-dir is set (useful to locate cached JSON)
}
"""

from __future__ import annotations

import argparse
import hashlib
import json
import os
import re
import sys
from typing import Any, Dict, List, Optional, Tuple

ADAPTER_VERSION = "2026-01-06_members_namespace_merge_v1"


def _esc_module(s: str) -> str:
    return s.replace("\\\\", "\\\\\\\\").replace("'", "\\\\'")


def build_any_dts(modules: Dict[str, Dict[str, Any]], *, pkg_name: str) -> str:
    chunks: List[str] = []
    chunks.append(f"// Auto-generated by {pkg_name} (TypeBERT adapter: stub backend)\n\n")
    for mod in sorted(modules.keys()):
        info = modules.get(mod) or {}
        default_import = bool(info.get("defaultImport"))
        named = sorted(set([x for x in (info.get("named") or []) if isinstance(x, str)]))
        type_named = sorted(set([x for x in (info.get("typeNamed") or []) if isinstance(x, str)]))
        members = info.get("members") or {}
        if not isinstance(members, dict):
            members = {}
        chunks.append(f"declare module '{_esc_module(mod)}' {{\n")
        if default_import:
            chunks.append("  const __default: any;\n  export default __default;\n")
        # generic anchor export (helps namespace import)
        chunks.append("  export const __any: any;\n")
        for n in named:
            if n.isidentifier():
                chunks.append(f"  export const {n}: any;\n")
        for t in type_named:
            if t.isidentifier():
                chunks.append(f"  export type {t} = any;\n")
        # Named import member access: emit namespace merging blocks (safe, conservative)
        for export_name, mems in sorted(members.items(), key=lambda kv: str(kv[0])):
            if not isinstance(export_name, str) or not export_name.isidentifier():
                continue
            if not isinstance(mems, list):
                continue
            mem_list = [m for m in mems if isinstance(m, str) and m.isidentifier()]
            if not mem_list:
                continue
            chunks.append(f"  export namespace {export_name} {{\n")
            for m in sorted(set(mem_list)):
                chunks.append(f"    export const {m}: any;\n")
            chunks.append("  }\n")
        chunks.append("}\n\n")
    return "".join(chunks)


def _stable_json_dumps(obj: Any) -> str:
    return json.dumps(obj, ensure_ascii=False, sort_keys=True, separators=(",", ":"))


def _extract_declare_module_blocks(text: str) -> str:
    """
    Extract `declare module 'x' { ... }` blocks using brace balancing.
    Regex-only extraction breaks easily because module bodies contain nested braces.
    """
    if not text:
        return ""

    # Find "declare module '<...>' {" occurrences.
    head_pat = re.compile(r"declare\s+module\s+(['\"])([^'\"]+)\1\s*\{", re.MULTILINE)

    def scan_block(start_idx: int) -> Optional[int]:
        """
        Given an index pointing at the '{' of the module head, return end index (exclusive)
        right after the matching closing '}' of that module, or None if unbalanced.
        Best-effort skipping of strings and comments.
        """
        i = start_idx
        depth = 0
        in_squote = False
        in_dquote = False
        in_btick = False
        in_line_comment = False
        in_block_comment = False
        escape = False
        n = len(text)
        while i < n:
            ch = text[i]
            nxt = text[i + 1] if i + 1 < n else ""

            if in_line_comment:
                if ch == "\n":
                    in_line_comment = False
                i += 1
                continue
            if in_block_comment:
                if ch == "*" and nxt == "/":
                    in_block_comment = False
                    i += 2
                    continue
                i += 1
                continue

            if in_squote or in_dquote or in_btick:
                if escape:
                    escape = False
                    i += 1
                    continue
                if ch == "\\":
                    escape = True
                    i += 1
                    continue
                if in_squote and ch == "'":
                    in_squote = False
                elif in_dquote and ch == '"':
                    in_dquote = False
                elif in_btick and ch == "`":
                    in_btick = False
                i += 1
                continue

            # Enter comments
            if ch == "/" and nxt == "/":
                in_line_comment = True
                i += 2
                continue
            if ch == "/" and nxt == "*":
                in_block_comment = True
                i += 2
                continue

            # Enter strings
            if ch == "'":
                in_squote = True
                i += 1
                continue
            if ch == '"':
                in_dquote = True
                i += 1
                continue
            if ch == "`":
                in_btick = True
                i += 1
                continue

            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return i + 1
                if depth < 0:
                    return None
            i += 1
        return None

    blocks: List[str] = []
    for m in head_pat.finditer(text):
        head_start = m.start()
        brace_idx = text.find("{", m.end() - 1)
        if brace_idx < 0:
            continue
        end_idx = scan_block(brace_idx)
        if end_idx is None:
            continue
        blocks.append(text[head_start:end_idx].rstrip() + "\n\n")

    return ("".join(blocks).strip() + "\n") if blocks else ""


def _rewrite_export_type_named_list(dts: str) -> str:
    """
    Fix invalid constructs like `export type { Foo, Bar }` (which expects `from`).
    Convert them into `export type Foo = any;` lines.
    """
    out_lines: List[str] = []
    pat = re.compile(r"^\s*export\s+type\s*\{\s*([^}]+?)\s*\}\s*;?\s*$")
    for ln in (dts or "").splitlines():
        m = pat.match(ln)
        if not m:
            out_lines.append(ln)
            continue
        inside = m.group(1)
        # Split by comma, support `A as B`
        for seg in inside.split(","):
            s = seg.strip()
            if not s:
                continue
            mm = re.match(r"^([A-Za-z_$][\w$]*)(\s+as\s+([A-Za-z_$][\w$]*))?$", s)
            if not mm:
                continue
            local = mm.group(3) or mm.group(1)
            out_lines.append(f"  export type {local} = any;")
    return "\n".join(out_lines).strip() + "\n"


def _rewrite_export_named_list(dts: str) -> str:
    """
    Fix invalid constructs like `export { Foo, Bar }` (which expects `from` in that syntax).
    Convert them into `export const Foo: any;` lines.
    """
    out_lines: List[str] = []
    pat = re.compile(r"^\s*export\s*\{\s*([^}]+?)\s*\}\s*;?\s*$")
    for ln in (dts or "").splitlines():
        m = pat.match(ln)
        if not m:
            out_lines.append(ln)
            continue
        inside = m.group(1)
        for seg in inside.split(","):
            s = seg.strip()
            if not s:
                continue
            # support `default as X` and `A as B`
            mm = re.match(r"^(default|[A-Za-z_$][\w$]*)(\s+as\s+([A-Za-z_$][\w$]*))?$", s)
            if not mm:
                continue
            local = mm.group(3) or mm.group(1)
            if local == "default":
                local = "__default"
            out_lines.append(f"  export const {local}: any;")
    return "\n".join(out_lines).strip() + "\n"


def _rewrite_export_type_colon_syntax(dts: str) -> str:
    """
    Fix invalid lines like `export type Foo: any;` -> `export type Foo = any;`
    (Some LLMs confuse type alias syntax with value annotation syntax.)
    """
    out_lines: List[str] = []
    pat = re.compile(r"^(\s*export\s+type\s+)([A-Za-z_$][\w$]*)(\s*):(\s*.+?)\s*;?\s*$")
    for ln in (dts or "").splitlines():
        m = pat.match(ln)
        if not m:
            out_lines.append(ln)
            continue
        prefix, name, _, rhs = m.group(1), m.group(2), m.group(3), m.group(4)
        out_lines.append(f"{prefix}{name} = {rhs.strip()};")
    return "\n".join(out_lines).strip() + "\n"


def _repair_common_backslash_escapes(dts: str) -> Tuple[str, Dict[str, Any]]:
    """
    Some model outputs contain literal backslash escapes like '\\n' in the raw text (not JSON-escaped).
    Those sequences are invalid in `.d.ts` in many positions and can trigger TS1127/TS1434.
    We conservatively repair them into actual newlines/tabs if they appear to be used as formatting.
    """
    meta: Dict[str, Any] = {}
    s = dts or ""
    # Heuristic: only repair when we see patterns that strongly indicate formatting escapes.
    if "\\ndeclare module" not in s and "{\\n" not in s and "}\\n" not in s:
        return s, meta

    meta["repaired_backslash_escapes"] = True
    # Normalize Windows-style first
    s = s.replace("\\r\\n", "\n")
    # Common escapes used for formatting in model outputs
    s = s.replace("\\n", "\n").replace("\\t", "  ").replace("\\r", "\n")
    return s, meta


def _sanitize_ambient_module_dts(dts: str, modules: Dict[str, Dict[str, Any]]) -> Tuple[str, Dict[str, Any]]:
    """
    Apply conservative rewrites to avoid emitting syntax that triggers TS1005/TS1109.
    If suspicious patterns remain, fall back to stub(any) to keep the pipeline runnable.
    """
    meta: Dict[str, Any] = {}
    s = dts or ""
    s1 = _rewrite_export_type_named_list(s)
    s2 = _rewrite_export_named_list(s1)
    s3 = _rewrite_export_type_colon_syntax(s2)
    s4, repair_meta = _repair_common_backslash_escapes(s3)
    if repair_meta:
        meta.update(repair_meta)
    s3 = s4

    # If the model still produced these constructs, we consider it unsafe.
    suspicious = []
    if "export type {" in s3:
        suspicious.append("export-type-braces")
    # `export {` is valid in some contexts, but we treat the brace-form as suspicious here.
    if re.search(r"^\s*export\s*\{", s3, re.MULTILINE):
        suspicious.append("export-braces")
    # Literal escape sequences remaining usually indicate broken formatting from the model.
    if "\\n" in s3 or "\\t" in s3:
        suspicious.append("literal-backslash-escapes")

    if suspicious:
        meta["fallback_reason"] = "suspicious-syntax:" + ",".join(suspicious)
        return build_any_dts(modules, pkg_name="evaluation/model/typebert_infer.py (fallback stub: suspicious syntax)"), meta

    return s3, meta

def _has_module_block(dts: str, mod: str) -> bool:
    if not dts:
        return False
    esc = re.escape(mod)
    pat = re.compile(rf"declare\s+module\s+['\"]{esc}['\"]\s*\{{", re.MULTILINE)
    return bool(pat.search(dts))


def _remove_modules_from_dts(dts: str, modules_to_remove: List[str]) -> str:
    """
    Remove `declare module '<specifier>' { ... }` blocks for specified modules.
    We rely on brace-balanced blocks produced by `_extract_declare_module_blocks`.
    """
    if not dts or not modules_to_remove:
        return (dts or "").strip() + ("\n" if dts else "")
    out = dts
    for mod in modules_to_remove:
        esc = re.escape(mod)
        # Best-effort: remove the whole declare module block for this specifier.
        # Works because our extracted blocks are contiguous and balanced.
        pat = re.compile(rf"declare\s+module\s+['\"]{esc}['\"]\s*\{{[\s\S]*?\}}\s*", re.MULTILINE)
        out = pat.sub("", out)
    return out.strip() + ("\n" if out.strip() else "")


def _parse_csv_list(s: str) -> List[str]:
    xs = []
    for part in (s or "").split(","):
        p = part.strip()
        if p:
            xs.append(p)
    # stable unique
    seen = set()
    out = []
    for x in xs:
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _patch_missing_modules_with_any(dts: str, modules: Dict[str, Dict[str, Any]]) -> Tuple[str, List[str]]:
    missing: List[str] = []
    for mod in sorted(modules.keys()):
        if not _has_module_block(dts, mod):
            missing.append(mod)
    if not missing:
        return (dts.strip() + "\n") if dts else "", []
    any_dts = build_any_dts({m: modules[m] for m in missing}, pkg_name="typebert_infer.py:fallback-any")
    any_dts = re.sub(r"^//[^\n]*\n\n", "", any_dts, flags=re.MULTILINE)
    merged = (dts.rstrip() + "\n\n" + any_dts).strip() + "\n"
    return merged, missing


def _patch_missing_exports_with_any(dts: str, modules: Dict[str, Dict[str, Any]]) -> Tuple[str, Dict[str, List[str]]]:
    """
    Ensure that for each requested module, all requested export names exist.
    This is critical for avoiding TS2339 regressions when the model forgets to emit
    some names (especially those added via member-access heuristics).
    We patch missing exports as `any` inside the corresponding `declare module` block.
    """

    if not dts:
        return "", {}

    added: Dict[str, List[str]] = {}

    # We'll scan/patch one module block at a time using brace balancing.
    head_pat = re.compile(r"declare\s+module\s+(['\"])([^'\"]+)\1\s*\{", re.MULTILINE)
    blocks = []

    def scan_block(start_idx: int) -> Optional[int]:
        i = start_idx
        depth = 0
        in_squote = False
        in_dquote = False
        in_btick = False
        in_line_comment = False
        in_block_comment = False
        escape = False
        n = len(dts)
        while i < n:
            ch = dts[i]
            nxt = dts[i + 1] if i + 1 < n else ""

            if in_line_comment:
                if ch == "\n":
                    in_line_comment = False
                i += 1
                continue
            if in_block_comment:
                if ch == "*" and nxt == "/":
                    in_block_comment = False
                    i += 2
                    continue
                i += 1
                continue

            if in_squote or in_dquote or in_btick:
                if escape:
                    escape = False
                    i += 1
                    continue
                if ch == "\\":
                    escape = True
                    i += 1
                    continue
                if in_squote and ch == "'":
                    in_squote = False
                elif in_dquote and ch == '"':
                    in_dquote = False
                elif in_btick and ch == "`":
                    in_btick = False
                i += 1
                continue

            if ch == "/" and nxt == "/":
                in_line_comment = True
                i += 2
                continue
            if ch == "/" and nxt == "*":
                in_block_comment = True
                i += 2
                continue

            if ch == "'":
                in_squote = True
                i += 1
                continue
            if ch == '"':
                in_dquote = True
                i += 1
                continue
            if ch == "`":
                in_btick = True
                i += 1
                continue

            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return i + 1
                if depth < 0:
                    return None
            i += 1
        return None

    # First, collect module blocks with their ranges.
    for m in head_pat.finditer(dts):
        head_start = m.start()
        mod = m.group(2)
        brace_idx = dts.find("{", m.end() - 1)
        if brace_idx < 0:
            continue
        end_idx = scan_block(brace_idx)
        if end_idx is None:
            continue
        blocks.append((head_start, end_idx, mod))

    # Patch from the end to preserve indices.
    out = dts
    for head_start, end_idx, mod in sorted(blocks, key=lambda x: x[0], reverse=True):
        info = modules.get(mod) or {}
        default_import = bool(info.get("defaultImport"))
        named = sorted(set([x for x in (info.get("named") or []) if isinstance(x, str) and x.isidentifier()]))
        type_named = sorted(set([x for x in (info.get("typeNamed") or []) if isinstance(x, str) and x.isidentifier()]))
        members = info.get("members") or {}
        if not isinstance(members, dict):
            members = {}
        if not default_import and not named and not type_named:
            continue

        block = out[head_start:end_idx]
        to_add: List[str] = []

        # Ensure __any anchor exists (helps namespace import patterns)
        if not re.search(r"^\s*export\s+const\s+__any\b", block, re.MULTILINE):
            to_add.append("  export const __any: any;")

        # Ensure default export exists if requested
        if default_import and not re.search(r"^\s*export\s+default\b", block, re.MULTILINE):
            to_add.append("  const __default: any;")
            to_add.append("  export default __default;")

        # Ensure value exports
        for n in named:
            if re.search(rf"^\s*export\s+(?:declare\s+)?(?:const|function|class|namespace|enum)\s+{re.escape(n)}\b", block, re.MULTILINE):
                continue
            if re.search(rf"^\s*export\s*\{{[^}}]*\b{re.escape(n)}\b[^}}]*\}}\s*;?\s*$", block, re.MULTILINE):
                continue
            to_add.append(f"  export const {n}: any;")

        # Ensure type exports
        for t in type_named:
            if re.search(rf"^\s*export\s+(?:declare\s+)?(?:type|interface|class|namespace|enum)\s+{re.escape(t)}\b", block, re.MULTILINE):
                continue
            if re.search(rf"^\s*export\s+type\s*\{{[^}}]*\b{re.escape(t)}\b[^}}]*\}}\s*;?\s*$", block, re.MULTILINE):
                continue
            to_add.append(f"  export type {t} = any;")

        # Ensure namespace-merging members for named imports (Foo.bar)
        for export_name, mems in sorted(members.items(), key=lambda kv: str(kv[0])):
            if not isinstance(export_name, str) or not export_name.isidentifier():
                continue
            if not isinstance(mems, list):
                continue
            mem_list = [m for m in mems if isinstance(m, str) and m.isidentifier()]
            if not mem_list:
                continue
            for member in sorted(set(mem_list)):
                # Search for already-declared member inside any `export namespace Foo { ... }` blocks.
                if re.search(
                    rf"export\s+namespace\s+{re.escape(export_name)}\b[\s\S]*?\bexport\s+const\s+{re.escape(member)}\b",
                    block,
                    re.MULTILINE,
                ):
                    continue
                to_add.append(f"  export namespace {export_name} {{ export const {member}: any; }}")

        if not to_add:
            continue

        # Insert before the final closing brace of the module block.
        insert_at = block.rfind("}")
        if insert_at <= 0:
            continue
        patched_block = block[:insert_at].rstrip() + "\n" + "\n".join(to_add) + "\n" + block[insert_at:]
        out = out[:head_start] + patched_block + out[end_idx:]
        added[mod] = to_add

    return out.strip() + "\n", added


def _build_prompt(modules: Dict[str, Dict[str, Any]]) -> str:
    compact = {
        m: {
            "defaultImport": bool((modules.get(m) or {}).get("defaultImport")),
            "named": sorted(set([x for x in ((modules.get(m) or {}).get("named") or []) if isinstance(x, str)])),
            "typeNamed": sorted(set([x for x in ((modules.get(m) or {}).get("typeNamed") or []) if isinstance(x, str)])),
            "members": (modules.get(m) or {}).get("members") or {},
        }
        for m in sorted(modules.keys())
    }
    modules_json = json.dumps(compact, ensure_ascii=False, indent=2)
    return (
        "You are a TypeScript declaration file generator.\n"
        "Task: Generate ambient module declarations for the listed npm module specifiers.\n"
        "Constraints:\n"
        "- Output ONLY TypeScript declaration text (no markdown).\n"
        "- For each module, output exactly one block: declare module '<specifier>' { ... }\n"
        "- Prefer conservative, compatibility-first types. If unsure, use 'any' locally (not globally).\n"
        "- Include exports for requested names:\n"
        "  - If a name is in 'named', export it as a value: export const <name>: <type>;\n"
        "  - If a name is in 'typeNamed', export it as a type: export type <name> = <type>;\n"
        "  - If defaultImport=true, add a default export.\n"
        "\n"
        "Input modules JSON:\n"
        f"{modules_json}\n"
        "\n"
        "Begin declarations now:\n"
    )


def _try_load_transformers() -> Tuple[Optional[Any], Optional[Any], Optional[str]]:
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore
    except Exception as e:
        return None, None, f"transformers import failed: {e}"
    return AutoModelForCausalLM, AutoTokenizer, None


def _infer_with_hf_causal_lm(
    *,
    model_name_or_path: str,
    prompt: str,
    device: str,
    max_new_tokens: int,
    temperature: float,
    seed: int,
    torch_dtype: str,
    low_cpu_mem_usage: bool,
    trust_remote_code: bool,
) -> Tuple[Optional[str], Dict[str, Any], Optional[str]]:
    meta: Dict[str, Any] = {
        "backend": "hf_causal_lm",
        "model": model_name_or_path,
        "device": device,
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "seed": seed,
        "torch_dtype": torch_dtype,
        "low_cpu_mem_usage": low_cpu_mem_usage,
        "trust_remote_code": trust_remote_code,
    }
    AutoModelForCausalLM, AutoTokenizer, err = _try_load_transformers()
    if err:
        return None, meta, err

    try:
        import torch  # type: ignore
    except Exception as e:
        return None, meta, f"torch import failed: {e}"

    if not model_name_or_path:
        return None, meta, "missing --model (or TYPEBERT_MODEL)"

    # Determinism best-effort
    try:
        torch.manual_seed(seed)
    except Exception:
        pass

    use_device = device
    if device == "auto":
        if torch.cuda.is_available():
            use_device = "cuda"
        elif getattr(torch.backends, "mps", None) is not None and torch.backends.mps.is_available():
            use_device = "mps"
        else:
            use_device = "cpu"
    meta["resolved_device"] = use_device

    try:
        dtype_obj = None
        if torch_dtype and torch_dtype != "auto":
            # common dtypes
            if torch_dtype == "float16":
                dtype_obj = torch.float16
            elif torch_dtype == "bfloat16":
                dtype_obj = torch.bfloat16
            elif torch_dtype == "float32":
                dtype_obj = torch.float32
        # reasonable default: use float16 on cuda/mps (memory), else float32
        if dtype_obj is None and torch_dtype == "auto":
            if use_device in ("cuda", "mps"):
                dtype_obj = torch.float16
            else:
                dtype_obj = torch.float32

        tok = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=trust_remote_code)
        model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            torch_dtype=dtype_obj,
            low_cpu_mem_usage=low_cpu_mem_usage,
            trust_remote_code=trust_remote_code,
        )
        model.eval()
        model.to(use_device)

        inputs = tok(prompt, return_tensors="pt")
        inputs = {k: v.to(use_device) for k, v in inputs.items()}

        do_sample = temperature > 0.0
        gen = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature if do_sample else None,
            pad_token_id=tok.eos_token_id,
            eos_token_id=tok.eos_token_id,
        )
        text = tok.decode(gen[0], skip_special_tokens=True)
        if text.startswith(prompt):
            text = text[len(prompt) :]
        return text.strip(), meta, None
    except Exception as e:
        return None, meta, f"model inference failed: {e}"


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--backend",
        default=os.environ.get("TYPEBERT_BACKEND", "stub"),
        choices=["stub", "hf_causal_lm", "typebert"],
        help="Inference backend (typebert is an alias of hf_causal_lm for backward compatibility)",
    )
    ap.add_argument("--model", default=os.environ.get("TYPEBERT_MODEL", ""), help="Local HF checkpoint path (recommended) or model id")
    ap.add_argument("--device", default=os.environ.get("TYPEBERT_DEVICE", "auto"), help="cpu|cuda|mps|auto")
    ap.add_argument("--max-new-tokens", type=int, default=int(os.environ.get("TYPEBERT_MAX_NEW_TOKENS", "800")))
    ap.add_argument("--temperature", type=float, default=float(os.environ.get("TYPEBERT_TEMPERATURE", "0.0")))
    ap.add_argument("--seed", type=int, default=int(os.environ.get("TYPEBERT_SEED", "0")))
    ap.add_argument(
        "--torch-dtype",
        default=os.environ.get("TYPEBERT_TORCH_DTYPE", "auto"),
        choices=["auto", "float16", "bfloat16", "float32"],
        help="Model dtype (default: auto; cuda/mps->float16, cpu->float32)",
    )
    ap.add_argument(
        "--low-cpu-mem-usage",
        default=os.environ.get("TYPEBERT_LOW_CPU_MEM_USAGE", "1"),
        choices=["0", "1"],
        help="Pass low_cpu_mem_usage=True to transformers (default: 1)",
    )
    ap.add_argument(
        "--trust-remote-code",
        default=os.environ.get("TYPEBERT_TRUST_REMOTE_CODE", "0"),
        choices=["0", "1"],
        help="Pass trust_remote_code=True for some models (default: 0)",
    )
    ap.add_argument(
        "--force-any-modules",
        default=os.environ.get("TYPEBERT_FORCE_ANY_MODULES", ""),
        help="Comma-separated module specifiers that should ALWAYS be stub(any) (model output is ignored for them).",
    )
    ap.add_argument("--cache-dir", default="", help="Optional cache dir for model outputs")
    args = ap.parse_args()

    raw = sys.stdin.read()
    try:
        req = json.loads(raw)
    except Exception as e:
        sys.stderr.write(f"failed to parse stdin json: {e}\\n")
        return 2

    modules = req.get("modules") or {}
    if not isinstance(modules, dict):
        modules = {}

    # cache (optional)
    cache_dir = args.cache_dir.strip()
    force_any_modules = _parse_csv_list(args.force_any_modules)
    cache_salt = {
        "adapter_version": ADAPTER_VERSION,
        "backend": args.backend,
        "model": args.model,
        "device": args.device,
        "max_new_tokens": args.max_new_tokens,
        "temperature": args.temperature,
        "seed": args.seed,
        "torch_dtype": args.torch_dtype,
        "low_cpu_mem_usage": args.low_cpu_mem_usage,
        "trust_remote_code": args.trust_remote_code,
        "force_any_modules": force_any_modules,
    }
    cache_key = hashlib.sha1((raw + "\n" + _stable_json_dumps(cache_salt)).encode("utf-8")).hexdigest()
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        cache_path = os.path.join(cache_dir, f"{cache_key}.json")
        if os.path.exists(cache_path):
            cached_txt = open(cache_path, "r", encoding="utf-8").read()
            # Backfill cache_key / repo for older cache files.
            try:
                cached_obj = json.loads(cached_txt)
                if isinstance(cached_obj, dict):
                    if cached_obj.get("cache_key") != cache_key:
                        cached_obj["cache_key"] = cache_key
                    if isinstance(cached_obj.get("meta"), dict) and "repo" not in cached_obj["meta"]:
                        cached_obj["meta"]["repo"] = req.get("repo")
                    cached_txt = json.dumps(cached_obj, ensure_ascii=False)
                    open(cache_path, "w", encoding="utf-8").write(cached_txt)
            except Exception:
                pass
            sys.stdout.write(cached_txt)
            return 0

    meta: Dict[str, Any] = {
        "requested_modules": len(modules),
        "adapter_version": ADAPTER_VERSION,
        "force_any_modules": force_any_modules,
        "repo": req.get("repo"),
    }

    backend = args.backend
    if backend == "typebert":
        backend = "hf_causal_lm"

    if backend == "hf_causal_lm":
        # For denylisted modules, we don't ask the model at all; we will always provide any stubs.
        modules_for_model = {k: v for (k, v) in modules.items() if k not in set(force_any_modules)}
        prompt = _build_prompt(modules_for_model)
        gen_text, m, err = _infer_with_hf_causal_lm(
            model_name_or_path=args.model,
            prompt=prompt,
            device=args.device,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            seed=args.seed,
            torch_dtype=args.torch_dtype,
            low_cpu_mem_usage=(args.low_cpu_mem_usage == "1"),
            trust_remote_code=(args.trust_remote_code == "1"),
        )
        meta.update(m)
        if err or not gen_text:
            dts = build_any_dts(modules, pkg_name="evaluation/model/typebert_infer.py (fallback stub)")
            meta["fallback_reason"] = err or "empty model output"
            out = {"ok": True, "backend": "stub", "dts": dts, "meta": meta}
        else:
            blocks = _extract_declare_module_blocks(gen_text)
            if not blocks:
                # If we cannot confidently extract valid ambient declarations, don't risk breaking tsc.
                dts = build_any_dts(modules, pkg_name="evaluation/model/typebert_infer.py (fallback stub: no declare module blocks)")
                meta["fallback_reason"] = "no-declare-module-blocks"
                out = {"ok": True, "backend": "stub", "dts": dts, "meta": meta}
            else:
                dts, sanitize_meta = _sanitize_ambient_module_dts(blocks, modules)
                if sanitize_meta.get("fallback_reason"):
                    meta.update(sanitize_meta)
                    out = {"ok": True, "backend": "stub", "dts": dts, "meta": meta}
                    out_json = json.dumps(out, ensure_ascii=False)
                    if cache_dir:
                        with open(cache_path, "w", encoding="utf-8") as f:
                            f.write(out_json)
                    sys.stdout.write(out_json)
                    return 0

                # Enforce force-any modules: remove any model-generated blocks for them,
                # then they will be filled in by the fallback-any patch step below.
                if force_any_modules:
                    dts = _remove_modules_from_dts(dts, force_any_modules)

                dts, missing = _patch_missing_modules_with_any(dts, modules)
                if missing:
                    meta["missing_modules_filled_with_any"] = missing
                dts, added_exports = _patch_missing_exports_with_any(dts, modules)
                if added_exports:
                    # Keep it compact: only store module names + counts
                    meta["missing_exports_filled_with_any"] = {k: len(v) for (k, v) in added_exports.items()}
                dts, repair_meta2 = _repair_common_backslash_escapes(dts)
                if repair_meta2:
                    meta["post_patch_repaired_backslash_escapes"] = True
                # If literal escapes still remain, do not risk breaking tsc.
                if "\\n" in dts or "\\t" in dts:
                    meta["fallback_reason"] = "literal-backslash-escapes-after-patch"
                    dts = build_any_dts(modules, pkg_name="evaluation/model/typebert_infer.py (fallback stub: escapes-after-patch)")
                    out = {"ok": True, "backend": "stub", "dts": dts, "meta": meta}
                else:
                    out = {"ok": True, "backend": "hf_causal_lm", "dts": dts, "meta": meta}
    else:
        dts = build_any_dts(modules, pkg_name="evaluation/model/typebert_infer.py")
        out = {"ok": True, "backend": "stub", "dts": dts, "meta": meta}

    if cache_dir:
        out["cache_key"] = cache_key

    out_json = json.dumps(out, ensure_ascii=False)
    if cache_dir:
        with open(cache_path, "w", encoding="utf-8") as f:
            f.write(out_json)
    sys.stdout.write(out_json)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


